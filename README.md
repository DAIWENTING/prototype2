# prototype2

1.	Software and Hardware Library Requirements
This module is written in Matlab. One addtional library (SVMLIB) is required, which can be found in 
https://www.csie.ntu.edu.tw/~cjlin/libsvm/. Whole process is run in standard PC.


2.	List of Files
1) data: a) prototype2_data: all image samples (Normal_Type-1, Normal_Type-2, Shifting, SolderBridge)
         b)	S1: Scenario1 
         c) S2:	Scenario2 
         d) S3:	Scenario3 
         e)	S4: Scenario4 
         f)README.txt: the description of 4 scenarios

2) our.m: the function of our proposed active learning method (self_training.m, clusterSelection.m,
and multiplySelection.m are the supporting function of our.m).

3) UI.m: the executable file of GUI. Running this file will begin the interacive demo.
4) UI.fig: the figure of the GUI
5) SVM_model.m, libsvmread.mexw64, libsvmwrite.mexw64, pdist2mex.mexw64, svmpredict.mexw64, svmtrain.mexw64: the files in SVM library.

3. Configuration Parameters
All parameters are fixed for evry experiment. 5% samples are used in initialization, 
and 5 samples will be selected in each active learning iteration.

4. Manual
•	The executable file is “UI.m”. 
•	After running it, please select a scenario first. The detailed description can be found in “README.txt” file in “data” folder.


•	After that, if you want to do labeling work by yourself, you can press “1 iteration” button to experience the real process the algorithm does during one iteration.
	The selected samples will be shown, you need to label them through the radio buttons and click “submit label” button.
	The automatic labels generated by anomaly detection are shown below the submit button. 

•	It also provides an automatic mode in “10 iterations”. All human labeling works are omitted.
•	Results and other information is shown on the right side.
Experiments setting
Experiment  Active Learning Method  Labels Getting From(in initialization, in active learning)
E1:   Random selection  Human Annotator  Human Annotator
E2:   our method  Human Annotator  Auto-labeling tool
E3:   our method  Human Annotator Human Annotator
E4:   our method  Auto-labeling tool Human Annotator
E5:   our method  Auto-labeling tool  Auto-labeling tool

The meaning of these numbers:
Human labeling (%/num): the percentage/number of samples labeled by human.
Auto_labeling (num/err): the number of samples automatically labeled by anomaly detection algorithm, and the number of false labels.
Accuracy (%): accuracy of the current classifier.



